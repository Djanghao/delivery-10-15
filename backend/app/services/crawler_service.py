from __future__ import annotations

import json
import logging
import time
import uuid
from dataclasses import dataclass
from datetime import datetime
from typing import Callable, Dict, Iterable, List, Optional

from sqlalchemy.orm import Session

from ..crawler.client import PublicAnnouncementClient
from ..crawler.models import ItemSummary, ProjectDetail
from ..models import CrawlProgress, CrawlRun, ValuableProject
from ..schemas import RegionNode
from .logs import append_log

logger = logging.getLogger(__name__)


@dataclass
class CrawlStats:
    total_items: int = 0
    valuable_projects: int = 0
    matched_projects: int = 0  # å‘½ä¸­çš„ç¬¦åˆæ¡ä»¶é¡¹ç›®ï¼ˆå«é‡å¤ï¼‰


class CrawlerService:
    def __init__(self, client: Optional[PublicAnnouncementClient] = None) -> None:
        self.client = client or PublicAnnouncementClient()

    def _build_region_name_map(self, region_codes: List[str]) -> Dict[str, str]:
        from ..config import DATA_DIR

        cache_file = DATA_DIR / "regions.json"
        if not cache_file.exists():
            return {code: code for code in region_codes}

        try:
            data = json.loads(cache_file.read_text(encoding="utf-8"))
        except Exception:
            return {code: code for code in region_codes}

        mapping: Dict[str, str] = {}
        parent_map: Dict[str, str] = {}

        def traverse(node: dict, parent_name: Optional[str] = None) -> None:
            node_id = node.get("id")
            node_name = node.get("name", node_id)
            if node_id:
                if parent_name and parent_name != node_name:
                    mapping[node_id] = f"{parent_name}/{node_name}"
                else:
                    mapping[node_id] = node_name
                parent_map[node_id] = parent_name or ""
            for child in node.get("children", []):
                traverse(child, node_name)

        for region in data:
            traverse(region)

        return {code: mapping.get(code, code) for code in region_codes}

    def fetch_region_tree(self) -> List[RegionNode]:
        regions = self.client.get_regions()
        nodes: Dict[str, RegionNode] = {
            region.id: RegionNode(id=region.id, name=region.name, parent_id=region.parent_id)
            for region in regions
        }
        for node in nodes.values():
            if node.parent_id and node.parent_id in nodes:
                nodes[node.parent_id].children.append(node)
        # è¿”å›å¸‚çº§ä½œä¸ºé¡¶å±‚ï¼ˆå»æ‰çœçº§ï¼‰ã€‚
        roots = [node for node in nodes.values() if not node.parent_id]
        if not roots:
            # å…œåº•ï¼šå¦‚æœæ²¡æœ‰æ ¹ç»“ç‚¹ï¼ŒæŒ‰åŸç»“æ„è¿”å›å…¨éƒ¨æ ¹ï¼ˆä¿æŒå…¼å®¹ï¼‰
            return [node for node in nodes.values() if not node.parent_id]
        city_level: List[RegionNode] = []
        for root in roots:
            # ä»…è¿”å›æ ¹ç»“ç‚¹ï¼ˆçœçº§ï¼‰çš„å­ç»“ç‚¹ï¼ˆå¸‚çº§ï¼‰
            city_level.extend(root.children)
        return city_level

    def run_task(
        self,
        session: Session,
        mode: str,
        region_codes: List[str],
        *,
        run_id: Optional[str] = None,
        should_stop: Optional[Callable[[], bool]] = None,
        exclude_keywords: str = "",
    ) -> CrawlRun:
        run_id = run_id or str(uuid.uuid4())
        crawl_run = CrawlRun(
            id=run_id,
            mode=mode,
            regions_json=json.dumps(region_codes),
            total_items=0,
            valuable_projects=0,
            started_at=datetime.utcnow(),
        )
        session.add(crawl_run)
        session.commit()
        region_name_map = self._build_region_name_map(region_codes)
        region_names = [region_name_map.get(code, code) for code in region_codes]
        append_log("INFO", f"ä»»åŠ¡ {run_id} å¼€å§‹ï¼Œæ¨¡å¼ {mode}ï¼Œåœ°åŒº {','.join(region_names)}")
        stats = CrawlStats()
        keywords_list = [kw.strip() for kw in exclude_keywords.split(",") if kw.strip()] if exclude_keywords else []
        if keywords_list:
            append_log("INFO", f"ä»»åŠ¡ {run_id} è¿‡æ»¤å…³é”®è¯: {', '.join(keywords_list)}")
        try:
            for region_code in region_codes:
                if should_stop and should_stop():
                    append_log("INFO", f"ä»»åŠ¡ {run_id} å·²è¯·æ±‚ç»ˆæ­¢ï¼Œåœæ­¢åç»­å¤„ç†")
                    break
                if mode == "history":
                    self._run_history_for_region(session, region_code, stats, region_name_map, should_stop=should_stop, exclude_keywords=keywords_list)
                else:
                    self._run_incremental_for_region(session, region_code, stats, region_name_map, should_stop=should_stop, exclude_keywords=keywords_list)
            crawl_run.total_items = stats.total_items
            crawl_run.valuable_projects = stats.valuable_projects
            crawl_run.finished_at = datetime.utcnow()
            session.commit()
            append_log(
                "INFO",
                f"ä»»åŠ¡ {run_id} å®Œæˆï¼šç´¯è®¡äº‹é¡¹ {stats.total_items}ï¼Œå‘½ä¸­ {stats.matched_projects}ï¼Œæ–°å…¥åº“ {stats.valuable_projects}"
            )
        except Exception as exc:
            crawl_run.finished_at = datetime.utcnow()
            session.commit()
            logger.exception("Crawl task failed")
            append_log("ERROR", f"ä»»åŠ¡ {run_id} å¤±è´¥: {exc}")
            raise
        return crawl_run

    def _run_history_for_region(
        self,
        session: Session,
        region_code: str,
        stats: CrawlStats,
        region_name_map: Dict[str, str],
        *,
        should_stop: Optional[Callable[[], bool]] = None,
        exclude_keywords: Optional[List[str]] = None,
    ) -> None:
        region_name = region_name_map.get(region_code, region_code)
        append_log("INFO", f"åœ°åŒº {region_name} å†å²çˆ¬å–å¼€å§‹")
        # å…ˆæ¢æµ‹æ€»é¡µæ•°ï¼ˆæ¥å£ pageNo æ”¯æŒä» 0 å¼€å§‹ï¼‰
        first_page = self.client.get_item_page(region_code, 0)
        total_pages = first_page.total_pages
        last_sendid = None
        before_region_total = stats.total_items
        before_region_matched = stats.matched_projects
        before_region_saved = stats.valuable_projects
        # å€’åºæŠ“å–ï¼šä»æœ€åä¸€é¡µç´¢å¼•(total_pages-1)åˆ°ç¬¬ 0 é¡µ
        for page_no in range(total_pages - 1, -1, -1):
            if should_stop and should_stop():
                append_log("INFO", f"åœ°åŒº {region_name} å†å²çˆ¬å–ä¸­æ­¢")
                break
            current_page = self.client.get_item_page(region_code, page_no)
            before_total = stats.total_items
            before_matched = stats.matched_projects
            before_saved = stats.valuable_projects
            self._process_items(session, region_code, reversed(current_page.items), stats, region_name_map, should_stop=should_stop, exclude_keywords=exclude_keywords)
            page_items = len(current_page.items)
            delta_total = stats.total_items - before_total
            delta_matched = stats.matched_projects - before_matched
            delta_saved = stats.valuable_projects - before_saved
            display_idx = page_no + 1
            if page_items or total_pages:
                append_log(
                    "INFO",
                    (
                        f"åœ°åŒº {region_name} å†å²çˆ¬å– - ç¬¬ {display_idx}/{total_pages} é¡µï¼Œ"
                        f"äº‹é¡¹ {page_items} æ¡ï¼Œå‘½ä¸­ {delta_matched} ä¸ªï¼Œæ–°å…¥åº“ {delta_saved} ä¸ª"
                    ),
                )
            if current_page.items:
                last_sendid = current_page.items[0].sendid
        if last_sendid:
            self._update_progress(session, region_code, last_sendid)
        region_total = stats.total_items - before_region_total
        region_matched = stats.matched_projects - before_region_matched
        region_saved = stats.valuable_projects - before_region_saved
        append_log("INFO", f"âœ“ åœ°åŒº {region_name} å†å²çˆ¬å–å®Œæˆï¼šç´¯è®¡äº‹é¡¹ {region_total} æ¡ï¼Œå‘½ä¸­ {region_matched} ä¸ªï¼Œæ–°å…¥åº“ {region_saved} ä¸ª")

    def _run_incremental_for_region(
        self,
        session: Session,
        region_code: str,
        stats: CrawlStats,
        region_name_map: Dict[str, str],
        *,
        should_stop: Optional[Callable[[], bool]] = None,
        exclude_keywords: Optional[List[str]] = None,
    ) -> None:
        region_name = region_name_map.get(region_code, region_code)
        append_log("INFO", f"åœ°åŒº {region_name} å¢é‡çˆ¬å–å¼€å§‹")
        progress = session.get(CrawlProgress, region_code)
        if not progress or not progress.last_pivot_sendid:
            append_log("INFO", f"åœ°åŒº {region_name} æ— å†å² pivotï¼Œæ‰§è¡Œå…¨é‡è¡¥é½")
            self._run_history_for_region(session, region_code, stats, region_name_map, should_stop=should_stop, exclude_keywords=exclude_keywords)
            return
        pivot = progress.last_pivot_sendid
        new_items = self._collect_items_after_pivot(region_code, pivot, region_name_map)
        if not new_items:
            append_log("INFO", f"âœ“ åœ°åŒº {region_name} å¢é‡çˆ¬å–å®Œæˆï¼šæ— æ–°å¢äº‹é¡¹")
            return
        before_matched = stats.matched_projects
        before_saved = stats.valuable_projects
        before_total = stats.total_items
        self._process_items(session, region_code, new_items, stats, region_name_map, should_stop=should_stop, exclude_keywords=exclude_keywords)
        delta_total = stats.total_items - before_total
        delta_matched = stats.matched_projects - before_matched
        delta_saved = stats.valuable_projects - before_saved
        latest = next(iter(new_items[::-1]), None)
        if latest:
            self._update_progress(session, region_code, latest.sendid)
        append_log("INFO", f"âœ“ åœ°åŒº {region_name} å¢é‡çˆ¬å–å®Œæˆï¼šç´¯è®¡äº‹é¡¹ {delta_total} æ¡ï¼Œå‘½ä¸­ {delta_matched} ä¸ªï¼Œæ–°å…¥åº“ {delta_saved} ä¸ª")

    def _collect_items_after_pivot(self, region_code: str, pivot: str, region_name_map: Dict[str, str]) -> List[ItemSummary]:
        region_name = region_name_map.get(region_code, region_code)
        items: List[ItemSummary] = []
        page_no = 0
        found = False
        total_pages = 0
        while True:
            page = self.client.get_item_page(region_code, page_no)
            total_pages = page.total_pages
            new_items_count = 0
            for entry in page.items:
                if entry.sendid == pivot:
                    found = True
                    break
                items.append(entry)
                new_items_count += 1
            display_idx = page_no + 1
            append_log(
                "INFO",
                f"åœ°åŒº {region_name} å¢é‡æ‰«æ - ç¬¬ {display_idx}/{total_pages} é¡µï¼Œæ–°å¢äº‹é¡¹ {new_items_count} æ¡"
                + (f"ï¼Œæ‰¾åˆ° pivot" if found else "")
            )
            if found or page_no >= (page.total_pages - 1):
                break
            page_no += 1
        if not found:
            append_log("WARNING", f"åœ°åŒº {region_name} æœªæ‰¾åˆ° pivot {pivot}ï¼Œè¿”å›æ‰€æœ‰äº‹é¡¹")
        return list(reversed(items))

    def _process_items(
        self,
        session: Session,
        region_code: str,
        items: Iterable[ItemSummary],
        stats: CrawlStats,
        region_name_map: Dict[str, str],
        *,
        should_stop: Optional[Callable[[], bool]] = None,
        exclude_keywords: Optional[List[str]] = None,
    ) -> None:
        region_name = region_name_map.get(region_code, region_code)
        empty_items_count = 0
        MAX_CONSECUTIVE_EMPTY = 10

        for item in items:
            if should_stop and should_stop():
                append_log("INFO", f"åœ°åŒº {region_name} é¡¹ç›®å¤„ç†è¢«ä¸­æ­¢")
                break
            stats.total_items += 1
            project = session.get(ValuableProject, item.projectuuid)
            if project:
                stats.matched_projects += 1
                self._update_progress(session, region_code, item.sendid)
                empty_items_count = 0
                continue
            retry_count = 0
            detail = None
            MAX_RETRIES = 50
            while retry_count < MAX_RETRIES:
                if should_stop and should_stop():
                    append_log("INFO", f"åœ°åŒº {region_name} é¡¹ç›®å¤„ç†è¢«ä¸­æ­¢ï¼ˆé¡¹ç›® {item.projectuuid}ï¼‰")
                    return
                try:
                    detail = self.client.get_project_detail(item.projectuuid)
                    if retry_count > 0:
                        append_log("INFO", f"âœ“ é¡¹ç›® {item.projectuuid} è·å–æˆåŠŸï¼ˆé‡è¯• {retry_count} æ¬¡åï¼‰")
                    break
                except Exception as exc:
                    retry_count += 1
                    if retry_count >= MAX_RETRIES:
                        append_log("ERROR", f"ğŸš¨ CRITICAL - é¡¹ç›® {item.projectuuid} è·å–å¤±è´¥ï¼ˆå·²é‡è¯•50æ¬¡ï¼‰ï¼Œè·³è¿‡")
                        detail = None
                        break
                    append_log("WARNING", f"âš ï¸ é¡¹ç›® {item.projectuuid} è·å–å¤±è´¥ï¼ˆç¬¬ {retry_count}/{MAX_RETRIES} æ¬¡é‡è¯•ï¼‰: {exc}")
                    time.sleep(2)
            if not detail:
                append_log("WARNING", f"é¡¹ç›® {item.projectuuid} æ— è¯¦æƒ…ï¼Œå¿½ç•¥")
                self._update_progress(session, region_code, item.sendid)
                continue

            if exclude_keywords:
                project_name = detail.project_name or ""
                should_skip = False
                for keyword in exclude_keywords:
                    if keyword in project_name:
                        append_log("INFO", f"ğŸš« è¿‡æ»¤é¡¹ç›®: {project_name} (åŒ¹é…å…³é”®è¯: {keyword})")
                        self._update_progress(session, region_code, item.sendid)
                        should_skip = True
                        break
                if should_skip:
                    continue

            if len(detail.items) == 0:
                empty_items_count += 1
                if empty_items_count >= MAX_CONSECUTIVE_EMPTY:
                    append_log("ERROR", f"ğŸš¨ çˆ¬å–ä¸­æ–­ - åœ°åŒº {region_name} è¿ç»­{MAX_CONSECUTIVE_EMPTY}ä¸ªé¡¹ç›®è¿”å›ç©ºäº‹é¡¹åˆ—è¡¨ï¼ŒåŸç½‘ç«™å¯èƒ½å‡ºç°é—®é¢˜")
                    return
            else:
                empty_items_count = 0
            if self._is_target_project(detail):
                project_uuid = detail.projectuuid or item.projectuuid
                session.merge(
                    ValuableProject(
                        projectuuid=project_uuid,
                        project_name=detail.project_name or "",
                        region_code=region_code,
                        discovered_at=datetime.utcnow(),
                    )
                )
                stats.valuable_projects += 1
                stats.matched_projects += 1
                append_log("INFO", f"è®°å½•é¡¹ç›® {detail.project_name}")
            self._update_progress(session, region_code, item.sendid)

    def _update_progress(self, session: Session, region_code: str, sendid: str) -> None:
        progress = session.get(CrawlProgress, region_code)
        if not progress:
            session.add(CrawlProgress(region_code=region_code, last_pivot_sendid=sendid))
        else:
            progress.last_pivot_sendid = sendid
        session.commit()

    @staticmethod
    def _is_target_project(detail: ProjectDetail) -> bool:
        return detail.is_target_project()
